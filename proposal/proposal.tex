\documentclass{article}

\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{chngpage}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{color}
\usepackage{graphicx,float,wrapfig}
\usepackage{inconsolata}
\usepackage{natbib}

% In case you need to adjust margins:
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

%\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex}

% Setup the header and footer
\pagestyle{fancy}
\lhead{Trevor Bekolay, Comprehensive II}
\rhead{\thepage\ of\ \protect\pageref{LastPage}}
\lfoot{}
\cfoot{}
\rfoot{}
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0pt}

% Make title
\title{Comp-II: Biologically plausible methods in speech recognition
  and synthesis}
\date{\today}
\author{\textbf{Trevor Bekolay}}

\begin{document}

\maketitle

\begin{abstract}
  Abstract
\end{abstract}

\section{Introduction}

\section{Speech recognition}

Automatic speech recognition
is a large and growing field
that has acted as a test bed
for new AI techniques for decades,
due to the allure of enabling
interactions with computers
using the same means
as humans interact with other humans.
Speech recognition can be formalized
with the following equation.
\begin{equation}
  \max_W p(W|O) = \max_W p(O|W) p(W),
\end{equation}
where $W$ is a sequence of words,
and $O$ is an audio waveform.
In this formalism, the speech recognition problem
is to find the most likely sequence of words
given some audio waveform.
Applying Bayes' rule, we can find
this sequence of words
if we know the probability
of some sequence of words occurring
in the target language,
and the distribution of audio waveforms
given some sequence of words.
In the literature, these two terms
are sometimes given explicit names.
The probabilities of a word sequence
occurring in a target language,
$p(W)$, is called the \textit{language model}
and the distribution of waveforms
given a word sequence, $p(O|W)$,
is called the \textit{acoustic model}.

In practice, these probability distributions
are too large to compute,
and instead the problem of speech recognition
is broken down into several subproblems
that are solved in sequence,
resulting in a speech recognition ``pipeline.''
While some techniques may span several levels
of the pipeline, at its finest granularity,
there are six levels to the pipeline,
as shown in Figure 1.

XXX: Insert figure 1 (like VanHoucke slide 22)

The pipeline begins with the \textit{audio waveform}.
This one-dimensional signal transformed
into a sequence of \textit{audio frames}
in a process typically called feature extraction.
In the case of audio signals,
speech recognition researchers have followed
biology by using the frequency spectrum
of the time-varying audio waveform
as the primary feature to be extracted.
The frequency components of the waveform
over a small time-window are often compressed,
resulting in each audio frame consisting of
a tractable number of coefficients
that describe the frequency components
of a slice of the audio waveform
(typically around 10 milliseconds of audio).
This transformation is typically done
with Fourier analysis (XXXcite),
which is sometimes distributed
to the Mel scale (XXXcite).
However, more sophisticated methods
such as Perceptual Linear Prediction (XXXcite),
Linear Predictive Coding (XXXcite),
and Cepstral analysis (XXXcite) have been
used to construct audio frames from a waveform.

Each audio frame is then transformed
into a \textit{state}
in a Hidden Markov Model (HMM).
This transformation is also called
\textit{acoustic modeling},
as we are building a statistical model
of the HMM states given the audio frame.
The first generation of successful
speech recognition systems
used Gaussian Mixture Models
to do efficient acoustic modeling.
However, modern speech recognition systems,
such as those used by Google,
use neural networks
(specifically, neural networks trained with deep learning)
for acoustic modeling.

Each state corresponds to a small window of time.
Sequences of states are transformed
into \textit{phonemes},
which are the smallest atoms of speech
studied in linguistics,
as they can be discriminated consciously by humans.
Mapping from states to phonemes
is typically done through a composition operation
by a finite state transducer (FST).
FSTs are heavily used in the higher levels
of speech recognition,
as they offer a mathematical formalism
that can be manipulated with efficient algorithms.

Phonemes are composed to form \textit{words}.
The phonemes that make up each word
can simply be looked up
in the target language's \textit{lexicon}.
In practice, the lexicon is often encoded
in a finite state transducer.

Words are composed to form \textit{sentences}.
While this starts to encroach
on higher-level features of language,
such as grammar and syntax,
most speech recognition systems
use simple statistical techniques
to determine if a sequence of words
forms a sentence.
Again, finite state transducers
are a common technique here,
though more sophisticated language models exist.

Little is known about the neurobiology
of speech recognition;
while several auditory pathways have been identified
in the human brain (XXXcite),
it is unknown if those pathways map onto
the speech recognition pipeline
used in artificial intelligence.
However, the first stage of the pipeline,
the mapping from audio waveforms
to audio frames, was originally inspired
by the auditory periphery system
in the brain.
Recent developments in auditory periphery modeling
have yet to be compared to the methods used
in artificial speech recognition,
and it may be possible that
these auditory periphery models
provide a better starting point
for the remaining steps in the pipeline
than existing techniques.

\subsection{Auditory periphery modelling}

While the functions of areas in the brain
associated with auditory processing
are largely unknown,
the auditory periphery is well understood.
Briefly, the auditory periphery
is responsible for sensing pressure waves
in the air and transducing them into
neural signals that we perceive as sound.
As seen in Figure~XXX,
air pressure waves are funneled into the auditory canal,
causing the tympanic membrane (eardrum)
to vibrate, which causes the stapes
to move in and out of the oval window of the cochlea.
Inside the cochlea, the movement of the stapes
disturbs fluid around the basilar membrane,
causing it to deform at specific points
depending on the frequency of the movement
of the stapes; higher frequency movements
cause deformation at the base of the basilar membrane,
and low frequency movements cause deformation
at the apex of the basilar membrane.
Hair cells lie on top of the basilar membrane
and touch the tectorial membrane.
As the basilar membrane is deformed,
the hair cells lie against
the tectorial membrane at different angles,
which cause different amounts of ions
to flow into the hair cell.
This directly controls the membrane potential
of the hair cells,
which is communicated
to the brain through cochlear nerve fibers.
Note that the hair cells exhibit graded potentials;
that is, they do not fire action potentials.
The hair cells synapse with spiral ganglion neurons,
whose axons form the auditory nerve
(also known as the cochlear nerve).

XXX: Insert Figure 2; auditory periphery basics

While the basic function of the auditory periphery
is to separate the incoming vibrations
into its frequency components,
as speech recognition systems do,
the actual transformation is significantly
more complicated than that.
Frequency components are nonlinearly weighted.
The incoming signal is amplified
if its amplitude is low,
but at a certain volume transitions
to a non-amplified state,
resulting in complicated dynamics
at the transition point.
These and several other nonlinearities
make auditory periphery modeling
significantly more complicated
than just Fourier analysis.

A computational model proposed originally by
Bruce, Sachs \& Young
and later extended by Zilany, Bruce, Nelson, \& Carney
has been able to replicate
many of the peculiarities
of the biological auditory periphery (XXXcites),
and has been used in cochlear implants
due to its ability to produce
similar signals as biological hair cells.
We will refer to this as the
Zilany model in the remainder of this proposal.

The Zilany model consists of a series
of interacting nonlinear filters
which produce a membrane potential
similar to that produced
by inner hair cells along the basilar membrane
(see Figure~XXX).
This is fed into a synapse model and spike generator
that emulates the action of cochlear cells,
and produces the action potentials
that travel along the auditory nerve.

XXX: Figure 2 from Zilany 2009

The instantaneous pressure waveform
(i.e., the audio input)
goes through a middle-ear filter,
whose output goes through three sets of filters
that are evaluated in parallel:
the C1, C2 and control path filters.
The middle-ear filter is a fifth order digital filter.
The control path is responsible for
setting the $\tau_{C1}$ time constant,
which affects the gain and bandwidth
of the C1 filter.
$\tau_{C1}$ is determined by
a series of nonlinear
(gammatone, Boltzmann)
and low-pass filters.
The C1 and C2 filters are
complex functions with three poles
(i.e., frequencies for which the
value of the function goes to infinity)
and one zero (i.e., a frequency
for which the value of the function goes to zero).
The locations of the poles and zero
are determined by
several parameters, including
the tuning properties of
the hair cell being simulated,
and its characteristic frequency,
which partially determines the cell's
tuning properties.
The C1 filter is responsible for most
low to medium frequency responses,
modeling both inner and outer hair cell
activity, resulting in a chirp filter.
The C2 is responsible for high frequency responses,
modeling inner hair cell activity
with a wideband filter;
the C2 filter is identical
to the broadest possible C1 filter,
which occurs when the outer hair cells
are completely impaired.
Finally, the C1 and C2 filters
are transduced to a voltage response,
summed together, and low-pass filtered
to obtain the final output voltage
of the inner hair cells.

The resulting voltage from the inner hair cell
is then communicated to a spiral ganglion cell
through the IHC-AN synapse.
This synapse exhibits adaptation dynamics
on at least three time scales:
rapid adaptation on the scale of milliseconds,
short-term adaptation on the scale of tens of milliseconds,
and slow adaptation on the scale of seconds
(XXXcite).
Zilany et al. model each time scale explicitly.
On the shortest time scale,
adaptation is exponential.
Exponential adaptation occurs naturally
through simulating the diffusion
of three stores of neurotransmitter
across the synaptic cleft.
On the medium and long timescales,
power-law adaptation is directly computed
by convolving a power-law kernel with
the history of prior responses;
the parameters of the kernel
are larger on the medium timescale
compared to the long timescale.
The results of the three timescales
are combined to give the output
in terms of the spontaneous rate
(spikes per second) of the cell being simulated.

The spontaneous rate,
as determined by the synapse model,
is mapped to spike times
through a nonhomogeneous Poisson process
modified to include refractory effects.
XXXciteZhang2001

\subsection{Open research questions}

While the Zilany model
has closely replicated experimental data,
and has proven useful in cochlear implants,
it has not yet been used in the context
of automatic speech recognition.

Can using auditory periphery signals
instead of waveforms
result in better speech recognition?

We'll try to do a small subset of speech recognition.

\section{Speech synthesis}

\subsection{Articulatory speech synthesis}

\subsection{Open research questions}

Can current vocal tract models give human-quality speech?

We'll try using modern motor control to generate target sounds.
We can use mngu0 sounds and fMRI for generating initial trajectories

\section{Closed loop modelling}

\subsection{Open research questions}

Can synthesized speech
influence auditory sensory learning,
and vice versa?

We'll try to hook them up together
and learn via babbling and outside teachers.

\section{Spiking neural networks}

\subsection{Open research questions}

Can the previously discussed items
be done in a biologically plausible manner?
Are there any practical benefits to doing this?

We'll use the NEF to model some aspects of this.
Some things,
like representing the articular parameters with neurons,
are trivial but provide good practical benefits
since computer speech sound mechanical due to no noise.

\subsection{Neural Engineering Framework}

\subsection{Learning}

\end{document}
