\documentclass{article}

\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{chngpage}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{color}
\usepackage{graphicx,float,wrapfig}
\usepackage{inconsolata}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage[top=1in,bottom=1in,left=1.5in,right=1.5in]{geometry}
\hypersetup{
  colorlinks = true, % Colours links instead of ugly boxes
  urlcolor = blue, % Colour for external hyperlinks
  linkcolor = blue, % Colour of internal links
  citecolor = blue % Colour of citations
}

%\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex}

% Setup the header and footer
\pagestyle{fancy}
\lhead{Trevor Bekolay, Comprehensive II}
\rhead{\thepage\ of\ \protect\pageref{LastPage}}
\lfoot{}
\cfoot{}
\rfoot{}
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0pt}

\graphicspath{{figures/}}

% Make title
\title{\bf Biologically plausible methods \\
  in speech recognition and synthesis: \\
  closing the loop}
\date{\today}
\author{{\bf Trevor Bekolay} \\
  Centre for Theoretical Neuroscience, University of Waterloo \\
  Waterloo, ON  N2L 3G1}

\begin{document}

\maketitle

\begin{abstract}
  Abstract
\end{abstract}

\section{Introduction}

\section{Background}

\subsection{Speech recognition}

Automatic speech recognition
is a large and growing field
that has acted as a test bed
for new AI techniques for decades,
due to the allure of enabling
interactions with computers
using the same means
as humans interact with other humans.
Speech recognition can be formalized
with the following equation.
\begin{equation}
  \max_W P(W|O) = \max_W P(O|W) P(W),
\end{equation}
where $W$ is a sequence of words,
and $O$ is an audio waveform.
In this formalism, the speech recognition problem
is to find the most likely sequence of words
given some audio waveform.
Applying Bayes' rule, we can find
this sequence of words
if we know the probability
of some sequence of words occurring
in the target language,
and the distribution of audio waveforms
given some sequence of words.
In the literature, these two terms
are sometimes given explicit names.
The probabilities of a word sequence
occurring in a target language,
$p(W)$, is called the \textit{language model}
and the distribution of waveforms
given a word sequence, $p(O|W)$,
is called the \textit{acoustic model}.

In practice, these probability distributions
are too large to compute,
and instead the problem of speech recognition
is broken down into several subproblems
that are solved in sequence,
resulting in a speech recognition ``pipeline.''
While some techniques may span several levels
of the pipeline, at its finest granularity,
there are six levels to the pipeline,
as shown in Figure~\ref{fig:recognition}.

\begin{figure}\begin{center}
  %\includegraphics[width=0.8\linewidth]{recognition}
  \label{fig:recognition}
  \caption{}
\end{center}\end{figure}

The pipeline begins with the \textit{audio waveform}.
This one-dimensional signal transformed
into a sequence of \textit{audio frames}
in a process typically called feature extraction.
In the case of audio signals,
speech recognition researchers have followed
biology by using the frequency spectrum
of the time-varying audio waveform
as the primary feature to be extracted.
The frequency components of the waveform
over a small time-window are often compressed,
resulting in each audio frame consisting of
a tractable number of coefficients
that describe the frequency components
of a slice of the audio waveform
(typically around 10 milliseconds of audio).
This transformation is typically done
with Fourier analysis,
which is sometimes distributed
to the Mel scale \citep{stevens1937}.
However, more sophisticated methods
such as Perceptual Linear Prediction \citep{hermansky1990},
Linear Predictive Coding \citep{oshaughnessy1988},
and Cepstral analysis \citep{furui1981} have been
used to construct audio frames from a waveform.

Each audio frame is then transformed
into a \textit{state}
in a Hidden Markov Model (HMM).
This transformation is also called
\textit{acoustic modeling},
as we are building a statistical model
of the HMM states given the audio frame.
The first generation of successful
speech recognition systems
used Gaussian Mixture Models
to do efficient acoustic modeling.
However, modern speech recognition systems,
such as those used by Google,
use neural networks
(specifically, neural networks trained with deep learning)
for acoustic modeling.

Each state corresponds to a small window of time.
Sequences of states are transformed
into \textit{phonemes},
which are the smallest atoms of speech
studied in linguistics,
as they can be discriminated consciously by humans.
Mapping from states to phonemes
is typically done through a composition operation
by a finite state transducer (FST).
FSTs are heavily used in the higher levels
of speech recognition,
as they offer a mathematical formalism
that can be manipulated with efficient algorithms.

Phonemes are composed to form \textit{words}.
The phonemes that make up each word
can simply be looked up
in the target language's \textit{lexicon}.
In practice, the lexicon is often encoded
in a finite state transducer.

Words are composed to form \textit{sentences}.
While this starts to encroach
on higher-level features of language,
such as grammar and syntax,
most speech recognition systems
use simple statistical techniques
to determine if a sequence of words
forms a sentence.
Again, finite state transducers
are a common technique here,
though more sophisticated language models exist.

Little is known about the neurobiology
of speech recognition;
while several auditory pathways have been identified
in the human brain \citep{scott2003},
it is unclear if those pathways map onto
the speech recognition pipeline
used in artificial intelligence.
However, the first stage of the pipeline,
the mapping from audio waveforms
to audio frames, was originally inspired
by the auditory periphery system
in the brain.
Recent developments in auditory periphery modeling
have yet to be compared to the methods used
in artificial speech recognition,
and it may be possible that
these auditory periphery models
provide a better starting point
for the remaining steps in the pipeline
than existing techniques.

\subsection{Auditory periphery modelling}

While the functions of areas in the brain
associated with auditory processing
are largely unknown,
the auditory periphery is well understood.
Briefly, the auditory periphery
is responsible for sensing pressure waves
in the air and transducing them into
neural signals that we perceive as sound.
As seen in Figure~\ref{fig:anatomy},
air pressure waves are funneled into the auditory canal,
causing the tympanic membrane (eardrum)
to vibrate, which causes the stapes
to move in and out of the oval window of the cochlea.
Inside the cochlea, the movement of the stapes
disturbs fluid around the basilar membrane,
causing it to deform at specific points
depending on the frequency of the movement
of the stapes; higher frequency movements
cause deformation at the base of the basilar membrane,
and low frequency movements cause deformation
at the apex of the basilar membrane.
Hair cells lie on top of the basilar membrane
and touch the tectorial membrane.
As the basilar membrane is deformed,
the hair cells lie against
the tectorial membrane at different angles,
which cause different amounts of ions
to flow into the hair cell.
This directly controls the membrane potential
of the hair cells,
which is communicated
to the brain through cochlear nerve fibers.
Note that the hair cells exhibit graded potentials;
that is, they do not fire action potentials.
The hair cells synapse with spiral ganglion neurons,
whose axons form the auditory nerve
(also known as the cochlear nerve).

\begin{figure}\begin{center}
  %\includegraphics[width=1\linewidth]{periphery-anatomy}
  \label{fig:anatomy}
  \caption{Anatomy of the auditory periphery.}
\end{center}\end{figure}

While the basic function of the auditory periphery
is to separate the incoming vibrations
into its frequency components,
as speech recognition systems do,
the actual transformation is significantly
more complicated than that.
Frequency components are nonlinearly weighted.
The incoming signal is amplified
if its amplitude is low,
but at a certain volume transitions
to a non-amplified state,
resulting in complicated dynamics
at the transition point.
These and several other nonlinearities
make auditory periphery modeling
significantly more complicated
than just Fourier analysis.

\subsubsection{Zilany model}

A computational model proposed originally by
Bruce, Sachs \& Young
and later extended by Zilany, Bruce, Nelson, \& Carney
has been able to replicate
many of the peculiarities
of the biological auditory periphery
\citep{bruce2003,zilany2006,zilany2007,zilany2009,zilany2014},
and has been used in cochlear implants
due to its ability to produce
similar signals as biological hair cells.
We will refer to this as the
Zilany model in the remainder of this proposal.

The Zilany model consists of a series
of interacting nonlinear filters
which produce a membrane potential
similar to that produced
by inner hair cells along the basilar membrane
(see Figure~\ref{fig:zilany}).
This is fed into a synapse model and spike generator
that emulates the action of cochlear cells,
and produces the action potentials
that travel along the auditory nerve.

\begin{figure}\begin{center}
  %\includegraphics[width=1\linewidth]{zilany}
  \label{fig:zilany}
  \caption{}
\end{center}\end{figure}

The instantaneous pressure waveform
(i.e., the audio input)
goes through a middle-ear filter,
whose output goes through three sets of filters
that are evaluated in parallel:
the C1, C2 and control path filters.
The middle-ear filter is a fifth order digital filter.
The control path is responsible for
setting the $\tau_{C1}$ time constant,
which affects the gain and bandwidth
of the C1 filter.
$\tau_{C1}$ is determined by
a series of nonlinear
(gammatone, Boltzmann)
and low-pass filters.
The C1 and C2 filters are
complex functions with three poles
(i.e., frequencies for which the
value of the function goes to infinity)
and one zero (i.e., a frequency
for which the value of the function goes to zero).
The locations of the poles and zero
are determined by
several parameters, including
the tuning properties of
the hair cell being simulated,
and its characteristic frequency,
which partially determines the cell's
tuning properties.
The C1 filter is responsible for most
low to medium frequency responses,
modeling both inner and outer hair cell
activity, resulting in a chirp filter.
The C2 is responsible for high frequency responses,
modeling inner hair cell activity
with a wideband filter;
the C2 filter is identical
to the broadest possible C1 filter,
which occurs when the outer hair cells
are completely impaired.
Finally, the C1 and C2 filters
are transduced to a voltage response,
summed together, and low-pass filtered
to obtain the final output voltage
of the inner hair cells.

The resulting voltage from the inner hair cell
is then communicated to a spiral ganglion cell
through the IHC-AN synapse.
This synapse exhibits adaptation dynamics
on at least three time scales:
rapid adaptation on the scale of milliseconds,
short-term adaptation on the scale of tens of milliseconds,
and slow adaptation on the scale of seconds.
Zilany et al. model each time scale explicitly.
On the shortest time scale,
adaptation is exponential.
Exponential adaptation occurs naturally
through simulating the diffusion
of three stores of neurotransmitter
across the synaptic cleft.
On the medium and long timescales,
power-law adaptation is directly computed
by convolving a power-law kernel with
the history of prior responses;
the parameters of the kernel
are larger on the medium timescale
compared to the long timescale.
The results of the three timescales
are combined to give the output
in terms of the spontaneous rate
(spikes per second) of the cell being simulated.

The spontaneous rate,
as determined by the synapse model,
is mapped to spike times
through a nonhomogeneous Poisson process
modified to include refractory effects.
\cite{zhang2001}.

\subsection{Speech synthesis}

Speech synthesis is the converse
of speech recognition;
given a sequence of words,
produce an audio waveform
that resembles a human
voicing that sequence of words.
The pipeline for synthesis is very similar
to the pipeline for recognition,
only in reverse:
sentences are broken down into words,
which are broken down into
their underlying phonemes;
this is typically called the
``frontend'' of a speech synthesis system.
The phonemes are then either
directly converted into audio waveforms,
or further broken down into
smaller states which are converted to audio waveforms;
this is typically called the ``backend''
of a speech synthesis system.

While speech recognition
has seen significant advances
and commercial success recently,
speech synthesis remains
unsatisfactory to the general public
despite recent advances.
Modern speech synthesizers can
produce natural-sounding speech,
but that speech is inflexible
and cannot be modified
to reflect the speaker's
emotional state or speaking style,
making it sounds prerecorded.
Synthesis techniques
that are more flexible exist,
but are often computationally intensive,
and often sound ``robotic,''
or too perfect,
due to the way in which
speech is produced.

Currently, the dominant speech synthesis
methods is concatenative (or unit selection) synthesis.
In concatenative synthesis, waveforms are generated
by selecting and concatenating audio samples
from a large database of recorded sounds
(``units,'' typically phonemes, but sometimes
smaller samples on the order of tens of milliseconds).
Since the units are recordings of real speech,
the result sounds natural.
Audio databases typically contain samples with
varying prosodic characteristics
(rhythm, stress, intonation)
allowing for words to be synthesized
using the underlying units
with the desired prosodic characteristics.
Machine learning techniques
are employed to efficiently search
the database of units and combine
the units together
(e.g., HMMs and Viterbi search,
as are used in speech recognition).
While the results are generally good
(concatenative synthesis is used in
the majority of commercially available
text-to-speech systems\footnote{See
  \url{http://www.theverge.com/2013/9/17/4596374/machine-language-how-siri-found-its-voice}.})
this technique is limited to the corpus
of sounds available in the database.
Additionally, there can be discontinuities
in the transition from one unit
to the next, making even the recorded
voice sample sound artificial.
Attempts to modify the concatenated
samples after the fact,
to change its prosodic characteristics,
for example, often require
spectral transformations that also
result in unnatural-sounding speech.

Advances in computing power
have made statistical parametric synthesis
techniques viable.
While concatenative synthesis
cobbles together speech using existing speech,
statistical parametric synthesis
addresses speech synthesis
as a generative statistical problem,
much in the way that
the speech recognition formalism does.
Like concatenative synthesis,
a large database of recorded speech is necessary,
but instead of using
raw samples from that database,
the database is used to generate
a set of model parameters.
The problem of speech synthesis
is represented as
\begin{equation}
  \hat{o} = \arg \max_o P(o|W, \hat{\lambda}),
\end{equation}
where $o$ is a set of speech parameters,
$W$ is a sequence of words,
and $\hat{\lambda}$ is a set of model parameters
resulting from training on the speech database.
The appropriate speech parameters
for the given word sequence, $\hat{o}$,
is fed through a series of filters
to generate synthesized speech.

\begin{wrapfigure}{l}{0.421\textwidth}
  \vspace{-30pt}
  \begin{center}
    %\includegraphics[width=0.42\textwidth]{synthesizer}
  \end{center}
  \vspace{-12pt}
  \label{fig:birkholz}
  \caption{Birkholz articulatory synthesizer}
  \vspace{-20pt}
\end{wrapfigure}

The most common method of implementing
statistical parametric synthesis
is to use an elaborate set of HMMs,
working at many different levels
and in parallel to model
all of the features necessary to produce
intelligible speech.
These features sets are significantly
more complicated than those
used in speech recognition;
in recognition, phonetic, prosodic,
and other contextual cues
can be collapsed into single HMM states,
whereas in synthesis,
all of these states must be explicitly tracked.
In one system \citep{zen2007},
there over a hundred parameters to be modeled.
Like similar techniques in speech recognition,
deep neural networks have begun
to replace HMMs, with promising results
\citep{zen2013}.
Despite these advances, concatenative systems
were still judged the most natural
at the 2012 Blizzard Challenge
(2013 results not available at this time).

\subsection{Articulatory speech synthesis}

The most flexible approach to speech synthesis
is articulatory speech synthesis.
Articulatory synthesis systems generate
audio waveforms by simulating
airflow through the human vocal tract.
While this method is
the most closely tied to biology,
and was used in the first fully automatic
text-to-speech synthesizer,
its computational complexity
and unnatural sounding results
have hindered its adoption
among academics and industry.
Additionally, accurate simulation
requires intricate knowledge
of how the human vocal tract
produces sounds,
which at present is incomplete.
However, unlike other methods,
articulatory speech synthesis
is able to test theories
resulting from the biological exploration
of sound production.

Generally, articulatory speech synthesizers
consist of three (typically independent) modules.
The first is a \textit{control model},
which generates vocal tract movements
by determining the desired positions
of the articulators in the vocal tract
(i.e., the degrees of freedom in the control problem).
The second is a \textit{vocal tract model},
which converts movements, or desired positions,
into vocal tract geometries in continuous time.
The third is an \textit{acoustic model},
which converts the geometries of the vocal tract
into acoustic signals (i.e., audio waveforms).
Each module has several classes of implementations.

One class of control models used in
articulatory speech synthesis
are segmental control models,
which work on a set of goals and constraints.
Given a phonological description
of the utterance to be synthesized,
segmental control models
produce a set of timestamps
(when a phoneme is to be voiced)
and a set of goals for some,
but not necessarily all,
of the articulators.
For example, to voice an \textit{a},
a segmental control model might say
that the \textit{a} starts at 0 ms
and ends at 100 ms,
and that the glottal aperture
must be at position 10
for that entire time window,
but that the tongue tip
should aim to reach position 0
at some point in that time window.
By specifying hard and soft constraints
on the vocal tract model,
segmental control can produce
realistic transitions between
phonemes simply by allowing
non-essential articulators to
remain in their current positions.

Another class of control models are
action based control models,
also known as gestural control models,
which define a set of discrete motor actions
(i.e., motor primitives in motor control literature)
that are combined to produce sounds according
to a phonemic and prosodic description.
This leads to several levels of description.
At the highest level,
the description to voice an \textit{a}
might be
``close the glottis
and open the velopharyngeal valve.''
This gets mapped down to a
high level motor plan,
which defines when the ``glottal closing''
and ``velopharyngeal valve opening''
actions occur and for how long.
This then is mapped down
to a lower level phonetic articulatory description
(using inverse dynamics)
that defines how each articulator
moves over time.

The control model is tightly coupled
with the vocal tract model.
The vocal tract model defines
the articulators that can be controlled,
and typically also provides some clue
for what articulator values
are appropriate for the
sound being synthesized.

One class of vocal tract models
are statistical models,
which extract relevant characteristics
from a large set of data,
mostly magnetic resonance imaging (MRI)
paired with audio recordings.
A full three-dimensional representation
of the patient's vocal tract
can be reconstructed with sufficient data,
and by using dimensionality reduction techniques
\citep[e.g.,]{badin2002},
the articulatory control parameters
can be of tractable dimensionality
while still producing a full
3D representation of a vocal tract.

Another class of vocal tract models
are biomechanical models,
which attempt to fully reconstruct
the physiology of the vocal tract.
This type of model begins
like the statistical model,
using MRI and other anatomical measurements
to develop a 3D representation
of the vocal tract.
However, instead of statistically
reducing the dimensionality
to determine the degrees of freedom
available to the control system,
these models also model how
muscle activations affect the shape
of the vocal tract
and use that as the articulator.

A final class of vocal tract models
are geometric models,
which generalize the overall
shapes observed in
MRI and anatomical studies.
A set of parameters
are used to generate a 3D model
of a vocal tract,
which has a set of articulators.
While these models are typically
less detailed than those
generated from precise
measurements of one subject's vocal tract,
it can be used to generate vocal tracts
from different subjects
(both male and female).
The parameters of the model
can be tuned to a specific speaker,
though the resulting vocal tract
will not be as detailed.

Finally, the acoustic model takes
a vocal tract representation
and simulates the airflow from
the lungs, through the vocal tract,
to the air pressure coming out
of the mouth (i.e., the audio waveform).
These models typically relate
the vocal tract to some other physical
device that has been modeled previously.
One set of models treat the vocal tract
as an analog transmission line,
which has been used to
simulate arterial blood flow.
Another set of models treat the vocal tract
as an analog electrical circuit.
A final set of models,
called finite element wave propagation models,
explicitly models the air pressure wave
as it travels through each section
of the vocal tract.
Hybrid technique combine one
of the above techniques
with an analogous transformation
in the frequency domain for added accuracy.
Within these systems, the glottis
and the type of noise generated
by air turbulence can be changed
to modify the final synthesized audio.

\begin{wrapfigure}{l}{0.39\textwidth}
  \vspace{-30pt}
  \begin{center}
    %\includegraphics[width=0.39\textwidth]{birholz-params}
  \end{center}
  \vspace{-24pt}
\end{wrapfigure}

\subsubsection{Birkholz synthesizer}

One of the most successful articulatory synthesizers
is one by Birtkholz et al,
(available at \url{http://www.vocaltractlab.de/};
we will refer to it as the Birkholz synthesizer)%
\footnote{An early version of the synthesizer
came second in a singing challenge
at Interspeech 2007; results available at
\url{http://www.interspeech2007.org/Technical/synthesis_of_singing_challenge.php}.}.
The Birkholz synthesizer uses
an analog electrical circuit transmission line
for generating acoustics,
a geometric model for the vocal tract,
and an action based control model.

While the full details of the acoustic model
and the vocal tract model are not
essential for the work proposed here,
it is useful to note
the 23 degrees of freedom
being controlled by Birkholz's
action based control model.

\begin{tabular}{llrrl}
\toprule
{\bf Name} & {\bf Description} & {\bf Min.} & {\bf Max} & {\bf Unit} \\
\midrule
$HX$ & Horizontal hyoid position & 0.0 & 1.0 & \\
$HY$ & Vertical hyoid position & -6.0 & -3.4 & cm \\
$JX$ & Horizontal jaw displacement & -0.5 & 0.0 & cm \\
$JA$ & Jaw angle & -7.0 & 0.0 & deg \\
$LP$ & Lip protrusion & -1.0 & 1.0 & \\
$LD$ & Vertical lip distance & -2.0 & 4.0 & cm \\
$VS$ & Velum shape & 0.0 & 1.0 & \\
$VO$ & Velic opening & -0.1 & 1.0 & \\
$TCX$ & Tongue body center X & -3.0 & 4.0 & cm \\
$TCY$ & Tongue body center Y & -3.0 & 1.0 & cm \\
$TTX$ & Tongue tip X & 1.5 & 5.5 & cm \\
$TTY$ & Tongue tip Y & -3.0 & 2.5 & cm \\
$TBX$ & Tongue blade X & -3.0 & 4.0 & cm \\
$TBY$ & Tongue blade Y & -3.0 & 5.0 & cm \\
$TRX$ & Tongue root X & -4.0 & 2.0 & cm \\
$TRY$ & Tongue root Y & -6.0 & 0.0 & cm \\
$TS1$ & Tongue side elevation 1 & -1.4 & 1.4 & cm \\
$TS2$ & Tongue side elevation 2 & -1.4 & 1.4 & cm \\
$TS3$ & Tongue side elevation 3 & -1.4 & 1.4 & cm \\
$TS4$ & Tongue side elevation 4 & -1.4 & 1.4 & cm \\
$MA1$ & Minimum area tongue back region & 0.0 & 0.3 & cm$^2$ \\
$MA2$ & Minimum area tongue tip region & 0.0 & 0.3 & cm$^2$ \\
$MA3$ & Minimum area lip region & 0.0 & 0.3 & cm$^2$ \\
\bottomrule
\end{tabular}

The actions generated from
the phonetic and prosodic representation
are mapped to actions in this 23-dimensional space
through analysis of MRI and dynamic MRI recordings
of a single speaker.
From that starting point,
each vowel phoneme was optimized
by minimizing the difference
between the simulated formants
and the speakers' formants\footnote{A
speech formant is a frequency which should
have maximum frequency for a specific
sound. Three such formants have been identified
for phonemes, though usually only the first
two are necessary to uniquely identify
a phoneme; for example, the long \textit{e}
vowel sound (as in fr\textit{ee})
can be uniquely identified with
formants $f_1=240$ Hz and $f_2=2400$ Hz.}
The optimizing procedure
was a greedy hill-climbing algorithm
in which each degree of freedom in
the control signal for a given vowel
was varied positively and negatively
and the formant difference was evaluated
until a local minimum was found.

Despite this optimization procedure
being relatively simple,
subjective error rates dropped dramatically,
pointing to the possibility
of greatly improved synthesis
by using more sophisticated optimization techniques.

\subsection{Motor control}

Articulatory speech synthesis
can be thought of as a
general motor control problem;
in the case of the Birkholz synthesizer,
the plant is the vocal tract model,
which has 23 degrees of freedom.
However, as an additional difficulty
above and beyond general motor control,
the measurable outcome of the plant
is a complex function of that
23-dimensional vector,
resulting in a scalar result
that is difficult to map
back to the plant movements.
However, thanks to MRI and other
imaging techniques,
Birkholz and other articulatory
synthesis researchers have
been able to discern
plant positions over time
for many utterances\footnote{This
data is available for free in many cases;
e.g., \url{http://www.mngu0.org/}}.

Recent advances in motor control
may be applied to the problem
of articulatory speech synthesis.
One such advancement is recent work
in the area of operational space control,
in which control is done in task space;
in the speech synthesis case,
this would mean that the phoneme being voiced
is the target of control,
not all the degrees of freedom of the plant
\citep{nakanishi2008}.
However, operation space control
is primarily done in the context
of rigid body dynamics systems;
adapting these algorithms
to the dynamics of the vocal tract
is nontrivial.
Recent approaches that learn
operational space control
may overcome those difficulties
\citep{peters2008}.

Another promising direction
for using modern motor control
in articulatory speech synthesis
is to use existing techniques and data
to define movement primitives,
and use Dynamic Movement Primitives
\citep{schaal2006} to flexibly combine them.
Especially promising about DMPs
is that they are also defined
for rhythmic motions,
like those found in the glottis
and other parts of the vocal tract.

There have been some efforts
to combine OSC, DMPs, and other motor control concepts
into a cohesive framework
for controlling arbitrary plants.
One such effort is being undertaken
by the CNRG;
I plan to leverage that work and adapt
it for the auditory domain.

\subsection{Closed loop modelling}

One of the biggest challenges
for using modern motor control algorithms
in the speech domain
is the need for feedback.
Even the most sophisticated
articulatory speech synthesis systems
do not use feedback to improve control;
instead, they use very detailed measurements
of the control parameters.
In order to control those parameters flexibly
(and in the same way that the brain controls them)
a control system should use feedback
to fine-tune or completely learn
the dynamics of the vocal tract.

Several lines of research have converged recently
to make such a closed loop model possible;
speech recognition can be done
with a biologically plausible frontend,
informing an articulatory speech synthesizer
that optimizes itself based on feedback.

A concrete hypothesis on how
this might be done was proposed by Kroger et al.
\citep{kroger2009}.
While their model uses less sophisticated
methods than would be necessary
to do state of the art speech
recognition and synthesis,
the overall structure of the model
(shown in Figure~\ref{fig:loop})
can serve as a good starting point
for a closed-loop speech model.
In particular, Kroger et al. have
noted several important pathways
that are likely to be essential
to a closed-loop model.
For example, they separate feedback
into auditory feedback,
which can train cortical areas
dedicated to high-level linguistic knowledge,
and somatosensory feedback,
which can train cortical areas
dedicated to low-level motor control.
Despite only using self-organizing maps,
Kroger et al. was able to train their model
to produce and perceive several phonemes
using a training method analogous
to human babbling.

\begin{figure}\begin{center}
  %\includegraphics[width=1\linewidth]{kroeger}
  \label{fig:loop}
  \caption{}
\end{center}\end{figure}

While Kroger et al.
do not term this structure
as hierarchical,
it can easily be phrased hierarchically
(e.g., the high-dimensional acoustic
signal is reduced to a lower-dimensional
auditory map).
Similar hierarchies have been explored
in other projects by members of the
Computational Neuroscience Research Group
\citep[e.g.,]{eliasmith2012,dewolf2011,rasmussen-inpress}.

Other models have been proposed
that use auditory feedback
to learn how to control
an articulatory speech synthesizer.
Warlaumont developed a spiking neural model
that used a reinforcement-modulated
spike timing dependent plasticity rule
to reward the neural patterns
responsible for producing
recognizable utterances
in a babbling context
\citep{warlaumont2012,warlaumont2013}.
Howard \& Messum developed
a model called Elija
that goes even further,
starting from babbling
and progressing to learning
the names of objects,
though these objects all had
single syllable names.
Their focus was on interaction
between a human caregiver
and the model through a microphone.

All three of these
(Kroger et al., Warlaumont, and \textit{Elija})
use early childhood acquisition of speech
to test their models,
rather than on using feedback
to refine adult speech synthesis.
The only study that I am aware of
that attempts to do closed-loop learning
on a sophisticated articulatory synthesizer
is Prom-on et al. which attempted
to minimize the error in frequency space
between a synthesized and target sound
using a greedy stochastic gradient method.
While they were able to produce
sounds with similar frequency spectrograms
as the target sounds starting from
random initial vocal tract positions
in the Birkholz synthesizer,
the results have not been quantified
and compared to the manual
synthesis techniques.
However, the study does provide
evidence that the Birkholz synthesizer's
control parameters can be learned.

\subsection{Spiking neural networks}

While the biologically inspired techniques
previously discussed have
promising characteristics,
implementing them on traditional computers
results in discrete and
computationally intensive algorithms.
Implementing them in the same way
that biological systems implement
them---i.e., with spiking neurons---may
provide benefits in terms of
computational complexity
(if implemented on massively parallel hardware)
and realism, due to the non-discrete
and noisy nature of biological computation.

Modelling the human auditory system
in spiking neural networks
rarely goes past the periphery,
which has been presented above.
However, studies of
song recognition and production
in birds has resulted in
detailed models well grounded
in biology, which may be analogous
to human speech recognition and production.
One key finding in this literature
is that a pathway in the avian brain
that is analogous to
the cortex-basal ganglia-thalamus loop
is active during song learning,
but not production,
just as we might expect
for a general well-trained motor action
in the human brain
\citep{fiete2008}.
Spiking neural models have been proposed
in the context of this pathway
that produce sequences of firing patterns,
as observed in the avian brain (see Figure~\ref{fig:birdsong}).

\begin{figure}\begin{center}
  %\includegraphics[width=0.9\linewidth]{birdsong}
  \label{fig:birdsong}
  \caption{}
\end{center}\end{figure}

These firing sequences become
a learned \textit{template} of the song.
The bird then attempts to mimic the song
by producing songs until it matches
the template song;
this process occurs without
external feedback,
suggesting that it is trying
to match the learned template specifically.
Models of this template-matching
typically do some type of neurally
implemented reinforcement learning.

Both of these processes---sequence generation
through a cortex-basal ganglia-thalamus loop,
and reinforcement learning in the same context---have
been explored by members of the CNRG,
specifically in this case by
the proposer of this research.

While these spiking models
replicate avian experimental results well,
we are only aware of one study that
has extrapolated these models
to the domain of human speech:
Yildiz et al. used a birdsong
circuit to do speech recognition
with an error rate comparable
to the HMM-based methods described previously.
However, the vocabulary of words was very small
(10 words, uttered by several different speakers),
and the model did not use spiking neurons.

\subsubsection{Neural Engineering Framework}

The models proposed in
artificial intelligence and control theory typically
have little in common with the spiking neural models
proposed by computational neuroscientists.
While AI models
can perform tasks that are impressive to humans,
they do so in a way
that is not biologically plausible,
often in a way that is specific
to the task being solved,
and often utilizes discrete approximations
that appear ``robotic,'' especially
in the context of speech.
Spiking neural models,
on the other hand,
do not perform tasks humans cannot do,
but are biologically plausible,
can generalize well, and operate in
continuous, noisy domains.

The Neural Engineering Framework (NEF; \citealp{eliasmith2003})
provides a set of tools to implement
AI and control theory models
in spiking neural networks.
The NEF defines three principles
through which to represent
information in spiking neural networks,
transform that information through the connections
between populations of spiking neurons,
and implement dynamics through
recurrent connectivity.

Using the NEF allows for more flexibility
than traditional spiking neural networks,
as the principles do not depend
on a particular neuron model,
and so rate-neuron models or even direct
dynamical system computations
can be simulated to validate models
before running a computationally intensive
spiking neuron simulation.

In short, the NEF provides
the benefits of both AI/control theory models
and computational neuroscience models.
The primary downsides to using the NEF
are additional complexity
in constructing the model
(the models are constrained in ways
that AI models are not),
and additional computational complexity.

\subsubsection{Learning}

Learning is not a core principle of the NEF;
yet, learning is a key component
of speech recognition and synthesis systems.
It is clear that the specifics
of a language are not (and cannot)
be hard-wired,
and many of the models
discussed above in this proposal
involve learning through
optimization or synaptic weight changes.

While not a core principle,
learning has been applied to models constructed
with the NEF several times successfully.
To do transformations, the NEF
optimizes the weights between two populations
by solving a least-squares minimization problem.
\citet{macneil2011} proposed
a learning rule that minimized the same
error online, as long as an appropriate
error signal is available.
Previously, I have combined that rule
with an unsupervised learning rule
and showed that the error minimization
can be done more effectively with
the unsupervised component
\citep{bekolay2013}.
Recently, Crawford has experimented
with other unsupervised rules
and implemented them
in a more scalable and efficient manner
\citep{crawford-inpress}.

\section{Proposal}

All of the above background
provides an optimistic long-term
goal for a biologically plausible
model of the speech system,
from the activity of sensory
hair cells in the ear
to the activation of
the muscles in the vocal tract.
Such a system would involve
a recognition hierarchy,
taking in high-dimensional auditory features
and mapping them to lower-dimensional
features like word sequences
and prosodic characteristics,
and a synthesis hierarchy,
mapping from the lower-dimensional features
to continuous control signals
manipulating high-dimensional parameters
of a simulated vocal tract.

XXX: Figure of the two hierarchies and interactions

Having both of these hierarchies
in a single unified system
allows for many feedback connections,
both within each hierarchy
and between them.
For example,
XXX go over a few possible interactions

This type of large-scale unified system
is a good long-term goal,
and research in biologically plausible
methods in speech should keep in mind
this type of unified system
as the details are filled in.
However, filling in all of the details
could take decades,
and so I propose to begin
with a minimal goal that still
requires the interaction of both hierarchies,
which can serve as a starting point
for future work towards a large-scale
unified model of human speech.

\subsection{Recognizing and articulating a small vocabulary}

The goal of the project is to
use a speech recognition system
to recognize a small vocabulary
(for example, ten digits)
and use the recognized and remembered
speech samples to train
an articulatory synthesizer
to voice the same small vocabulary.
The main challenges in this project
are determining how to connect
existing systems together
and choosing representations that
can be communicated between systems.
The primary contribution of this project
would be a method to train an
articulatory synthesizer
to voice a small vocabulary
without requiring a large corpus
of experimental data
in order to determine control signals.

XXX: figure for proposed implementation

Figure XXX proposed an implementation
to achieve the goal of recognizing
and articulating a small vocabulary.
This implementation is inspired by
how songbirds learn to voice
learned songs (as summarized in Section~\ref{XXX}).
In the first stage, the words from
the vocabulary are fed to the
recognition hierarchy.
The job of the recognition hierarchy
is to classify the incoming
audio waveforms into the word
being voiced,
and to build up a representation
of what those words sounds like;
we will refer to this as
\textit{template learning},
borrowing the terminology
from the birdsong literature.
In the second stage, the templates
are used to train the
synthesis hierarchy.
The job of the synthesis hierarchy
is to voice the learned template words
as accurately as possible.
This will be done by replaying
the learned templates and
exploring the control parameter space
to determine control signals
that will match the learned templates;
we will refer to this as
\textit{motor learning}.

\subsubsection{Template learning}

We propose a template learning system
with three key components.
The first is to use the Zilany
auditory periphery model
as a frontend.
This model is described in more detail
in section~\ref{XXX}.
While it has not yet been shown
that this model will produce features
that make recognition easier or more efficient,
we believe that using a model
that produces spikes
(which can be converted into
instantaneous rates if desired)
leaves open more possibilities
for future model extensions.
The second component of the model
is a classification system
to categorize auditory features into digits,
and the third component
is a path integration system
to remember and replay the
remembered auditory features
of each digit.

The classification system will be based
on previous work XXXciteCogSci.
In that work, we used visual features
of digits to classify them
using a biologically plausible learning rule
operating in a spiking neural network
created using the principles of the NEF.
In this work, we will use auditory features,
which (unlike the visual features used previously)
change over time.
While we believe that temporal information
will improve the accuracy of classification,
incorporating temporal information
into the existing system will be a challenge.

The path integration model will be based
on an existing line of models
developed by XXXciteConklinEliasmith
and an ongoing extension
by Trujillo (unpublished).
In these path integration models,
a population of neurons tracks
the spatial location of the subject
over time.
In this work, we will track
auditory features over time
instead of spatial location;
however, the model scales up naturally
to arbitrary dimensionality,
so the model should not need
significant modifications.

With these three systems,
we propose a
two stage template learning system.
In the first stage,
we train the classification system
to recognize spoken digits
given the auditory features
produced by the Zilany model.
Once digits can be accurately classified,
we then build up a path through
auditory feature space for each digit.
Upon hearing a new utterance,
the classification system
would determine the digit being uttered;
the path integration system
would then replay the auditory features
of that digit utterance
and incorporate it into
a ``template'' digit utterance,
which would be similar to the
average of all utterances
the system has heard
of that particular digit.

An important possible extension to
this template learning system
(which are not likely to be
part of the proposed work)
is to make each learning stage
hierarchical.
The proposed system is likely
to work well for a small vocabulary,
but it is not feasible to learn
every word in this way.
Instead, it is likely that
we start off by mapping
from auditory features to words,
but later on develop a hierarchical mapping
from auditory features to phonemes,
which in turn map to parts of words or words.
This would allow for fast learning
of new words by simply integrating
a path through phoneme space.

\subsubsection{Motor learning}

The goal of the motor learning system
is to control the Birkholz articulatory synthesizer
such that it can match
the template digit utterances learned
by the template learning system.
Unlike other control models, however,
this system will start with
no prior knowledge
of the ``correct'' articulator positions
(through MRI data, for example),
and must instead learn
to produce the target sounds
through auditory feedback.

The motor learning system will
attempt to match the learned template
through repeated trials.
On each trail,
a control signal is generated,
which controls the Birkholz
articulatory synthesizer.
The audio waveform produced
by the synthesizer is fed
into the Zilany auditory model,
and the auditory features
of the synthesized sound
are compared to the template sound
to produce an error signal.
The motor system attempts
to minimize this error signal
on each subsequent trial.
Using methods from
reinforcement learning,
we will explore the control parameter space
and find a suitable trajectory
through that space
such that the error is minimized,
which occurs when the synthesizer
utters something that closely matches
the remembered template utterance.
Much like the path integration system
is remembering a trajectory through
auditory feature space,
rather than physical space,
this reinforcement learning system
is learning a trajectory through
articulatory parameter space,
rather than physical space.

There are a few ways to speed up
the learning process that I plan to explore.
The first speedup is to prematurely end trials
if a certain amount of error has accumulated.
A significant amount of learning time will
be devoted to producing the first correct phoneme,
and so it is more efficient to continually
attempt to voice that phoneme
than it is to continually attempt to voice
all of the phonemes.
Additionally, if that phoneme is not voiced properly,
any sounds produced from that point on
may not be voiced in the correct context;
for example, the vocal tract is in a certain
configuration before voicing the `n'
in `one' than before voicing the `n' in `an.'
Learning to voice a certain phoneme incorrectly
may need to be unlearned, adding more time.
The second speedup is to constrain the learning
by fixing visible articulators to be
the correct values during articulation.
Some articulators like lip and jaw positions
can be determined visually,
and it is plausible that when attempting
to match remembered templates
that those articular positions are also
remembered and replayed.
The third speedup is to
do some pretraining to determine
how changing each control parameter
affects the resulting audio waveform.
XXX: Get that paper from Chris about solving for control params
and fill out paragraph

As with the template learning system,
and important extension of this work
would be to learn to voice
the utterances in a hierarchical manner.
Rather than learning
to utter each template digit,
a hierarchical system would instead
learn how to utter each phoneme,
and plan trajectories through phoneme space.
However, for the small vocabulary proposed,
this is not necessary.

\subsection{Challenges}

XXX: Fill out as I do a full edit...

How to train the template classifier?
Do we give an explicit label, or just an arbitrary vector
(analogous to some external signal
representing the concept of each digit)?
Or do we do this in a completely unsupervised manner?

What are the best chunks to replay for the motor learning system?
Is there any way to transfer learning across the digits
without using hierarchical ideas?

We still aren't good at using reinforcement learning to go through a
2D continuous space; the 23-dimensional articulatory control parameter
space is much bigger than two dimensions...

\subsection{Contributions}

XXX: Fill out as I do a full edit...

\bibliographystyle{plainnat}
\bibliography{proposal}

\end{document}
