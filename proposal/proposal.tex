\documentclass{article}

\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{chngpage}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{color}
\usepackage{graphicx,float,wrapfig}
\usepackage{inconsolata}
\usepackage{natbib}
\usepackage[top=1in,bottom=1in,left=1.5in,right=1.5in]{geometry}

%\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex}

% Setup the header and footer
\pagestyle{fancy}
\lhead{Trevor Bekolay, Comprehensive II}
\rhead{\thepage\ of\ \protect\pageref{LastPage}}
\lfoot{}
\cfoot{}
\rfoot{}
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0pt}

% Make title
\title{\bf Biologically plausible methods \\
  in speech recognition and synthesis: \\
  closing the loop}
\date{\today}
\author{{\bf Trevor Bekolay} \\
  Centre for Theoretical Neuroscience, University of Waterloo \\
  Waterloo, ON  N2L 3G1}

\begin{document}

\maketitle

\begin{abstract}
  Abstract
\end{abstract}

\section{Introduction}

\section{Background}

\subsection{Speech recognition}

Automatic speech recognition
is a large and growing field
that has acted as a test bed
for new AI techniques for decades,
due to the allure of enabling
interactions with computers
using the same means
as humans interact with other humans.
Speech recognition can be formalized
with the following equation.
\begin{equation}
  \max_W P(W|O) = \max_W P(O|W) P(W),
\end{equation}
where $W$ is a sequence of words,
and $O$ is an audio waveform.
In this formalism, the speech recognition problem
is to find the most likely sequence of words
given some audio waveform.
Applying Bayes' rule, we can find
this sequence of words
if we know the probability
of some sequence of words occurring
in the target language,
and the distribution of audio waveforms
given some sequence of words.
In the literature, these two terms
are sometimes given explicit names.
The probabilities of a word sequence
occurring in a target language,
$p(W)$, is called the \textit{language model}
and the distribution of waveforms
given a word sequence, $p(O|W)$,
is called the \textit{acoustic model}.

In practice, these probability distributions
are too large to compute,
and instead the problem of speech recognition
is broken down into several subproblems
that are solved in sequence,
resulting in a speech recognition ``pipeline.''
While some techniques may span several levels
of the pipeline, at its finest granularity,
there are six levels to the pipeline,
as shown in Figure 1.

XXX: Insert figure 1 (like VanHoucke slide 22)

The pipeline begins with the \textit{audio waveform}.
This one-dimensional signal transformed
into a sequence of \textit{audio frames}
in a process typically called feature extraction.
In the case of audio signals,
speech recognition researchers have followed
biology by using the frequency spectrum
of the time-varying audio waveform
as the primary feature to be extracted.
The frequency components of the waveform
over a small time-window are often compressed,
resulting in each audio frame consisting of
a tractable number of coefficients
that describe the frequency components
of a slice of the audio waveform
(typically around 10 milliseconds of audio).
This transformation is typically done
with Fourier analysis (XXXcite),
which is sometimes distributed
to the Mel scale (XXXcite).
However, more sophisticated methods
such as Perceptual Linear Prediction (XXXcite),
Linear Predictive Coding (XXXcite),
and Cepstral analysis (XXXcite) have been
used to construct audio frames from a waveform.

Each audio frame is then transformed
into a \textit{state}
in a Hidden Markov Model (HMM).
This transformation is also called
\textit{acoustic modeling},
as we are building a statistical model
of the HMM states given the audio frame.
The first generation of successful
speech recognition systems
used Gaussian Mixture Models
to do efficient acoustic modeling.
However, modern speech recognition systems,
such as those used by Google,
use neural networks
(specifically, neural networks trained with deep learning)
for acoustic modeling.

Each state corresponds to a small window of time.
Sequences of states are transformed
into \textit{phonemes},
which are the smallest atoms of speech
studied in linguistics,
as they can be discriminated consciously by humans.
Mapping from states to phonemes
is typically done through a composition operation
by a finite state transducer (FST).
FSTs are heavily used in the higher levels
of speech recognition,
as they offer a mathematical formalism
that can be manipulated with efficient algorithms.

Phonemes are composed to form \textit{words}.
The phonemes that make up each word
can simply be looked up
in the target language's \textit{lexicon}.
In practice, the lexicon is often encoded
in a finite state transducer.

Words are composed to form \textit{sentences}.
While this starts to encroach
on higher-level features of language,
such as grammar and syntax,
most speech recognition systems
use simple statistical techniques
to determine if a sequence of words
forms a sentence.
Again, finite state transducers
are a common technique here,
though more sophisticated language models exist.

Little is known about the neurobiology
of speech recognition;
while several auditory pathways have been identified
in the human brain (XXXcite),
it is unknown if those pathways map onto
the speech recognition pipeline
used in artificial intelligence.
However, the first stage of the pipeline,
the mapping from audio waveforms
to audio frames, was originally inspired
by the auditory periphery system
in the brain.
Recent developments in auditory periphery modeling
have yet to be compared to the methods used
in artificial speech recognition,
and it may be possible that
these auditory periphery models
provide a better starting point
for the remaining steps in the pipeline
than existing techniques.

\subsection{Auditory periphery modelling}

While the functions of areas in the brain
associated with auditory processing
are largely unknown,
the auditory periphery is well understood.
Briefly, the auditory periphery
is responsible for sensing pressure waves
in the air and transducing them into
neural signals that we perceive as sound.
As seen in Figure~XXX,
air pressure waves are funneled into the auditory canal,
causing the tympanic membrane (eardrum)
to vibrate, which causes the stapes
to move in and out of the oval window of the cochlea.
Inside the cochlea, the movement of the stapes
disturbs fluid around the basilar membrane,
causing it to deform at specific points
depending on the frequency of the movement
of the stapes; higher frequency movements
cause deformation at the base of the basilar membrane,
and low frequency movements cause deformation
at the apex of the basilar membrane.
Hair cells lie on top of the basilar membrane
and touch the tectorial membrane.
As the basilar membrane is deformed,
the hair cells lie against
the tectorial membrane at different angles,
which cause different amounts of ions
to flow into the hair cell.
This directly controls the membrane potential
of the hair cells,
which is communicated
to the brain through cochlear nerve fibers.
Note that the hair cells exhibit graded potentials;
that is, they do not fire action potentials.
The hair cells synapse with spiral ganglion neurons,
whose axons form the auditory nerve
(also known as the cochlear nerve).

XXX: Insert Figure 2; auditory periphery basics

While the basic function of the auditory periphery
is to separate the incoming vibrations
into its frequency components,
as speech recognition systems do,
the actual transformation is significantly
more complicated than that.
Frequency components are nonlinearly weighted.
The incoming signal is amplified
if its amplitude is low,
but at a certain volume transitions
to a non-amplified state,
resulting in complicated dynamics
at the transition point.
These and several other nonlinearities
make auditory periphery modeling
significantly more complicated
than just Fourier analysis.

\subsubsection{Zilany model}

A computational model proposed originally by
Bruce, Sachs \& Young
and later extended by Zilany, Bruce, Nelson, \& Carney
has been able to replicate
many of the peculiarities
of the biological auditory periphery (XXXcites),
and has been used in cochlear implants
due to its ability to produce
similar signals as biological hair cells.
We will refer to this as the
Zilany model in the remainder of this proposal.

The Zilany model consists of a series
of interacting nonlinear filters
which produce a membrane potential
similar to that produced
by inner hair cells along the basilar membrane
(see Figure~XXX).
This is fed into a synapse model and spike generator
that emulates the action of cochlear cells,
and produces the action potentials
that travel along the auditory nerve.

XXX: Figure 2 from Zilany 2009

The instantaneous pressure waveform
(i.e., the audio input)
goes through a middle-ear filter,
whose output goes through three sets of filters
that are evaluated in parallel:
the C1, C2 and control path filters.
The middle-ear filter is a fifth order digital filter.
The control path is responsible for
setting the $\tau_{C1}$ time constant,
which affects the gain and bandwidth
of the C1 filter.
$\tau_{C1}$ is determined by
a series of nonlinear
(gammatone, Boltzmann)
and low-pass filters.
The C1 and C2 filters are
complex functions with three poles
(i.e., frequencies for which the
value of the function goes to infinity)
and one zero (i.e., a frequency
for which the value of the function goes to zero).
The locations of the poles and zero
are determined by
several parameters, including
the tuning properties of
the hair cell being simulated,
and its characteristic frequency,
which partially determines the cell's
tuning properties.
The C1 filter is responsible for most
low to medium frequency responses,
modeling both inner and outer hair cell
activity, resulting in a chirp filter.
The C2 is responsible for high frequency responses,
modeling inner hair cell activity
with a wideband filter;
the C2 filter is identical
to the broadest possible C1 filter,
which occurs when the outer hair cells
are completely impaired.
Finally, the C1 and C2 filters
are transduced to a voltage response,
summed together, and low-pass filtered
to obtain the final output voltage
of the inner hair cells.

The resulting voltage from the inner hair cell
is then communicated to a spiral ganglion cell
through the IHC-AN synapse.
This synapse exhibits adaptation dynamics
on at least three time scales:
rapid adaptation on the scale of milliseconds,
short-term adaptation on the scale of tens of milliseconds,
and slow adaptation on the scale of seconds
(XXXcite).
Zilany et al. model each time scale explicitly.
On the shortest time scale,
adaptation is exponential.
Exponential adaptation occurs naturally
through simulating the diffusion
of three stores of neurotransmitter
across the synaptic cleft.
On the medium and long timescales,
power-law adaptation is directly computed
by convolving a power-law kernel with
the history of prior responses;
the parameters of the kernel
are larger on the medium timescale
compared to the long timescale.
The results of the three timescales
are combined to give the output
in terms of the spontaneous rate
(spikes per second) of the cell being simulated.

The spontaneous rate,
as determined by the synapse model,
is mapped to spike times
through a nonhomogeneous Poisson process
modified to include refractory effects.
XXXciteZhang2001

\subsection{Speech synthesis}

Speech synthesis is the converse
of speech recognition;
given a sequence of words,
produce an audio waveform
that resembles a human
voicing that sequence of words.
The pipeline for synthesis is very similar
to the pipeline for recognition,
only in reverse:
sentences are broken down into words,
which are broken down into
their underlying phonemes;
this is typically called the
``frontend'' of a speech synthesis system.
The phonemes are then either
directly converted into audio waveforms,
or further broken down into
smaller states which are converted to audio waveforms;
this is typically called the ``backend''
of a speech synthesis system.

While speech recognition
has seen significant advances
and commercial success recently,
speech synthesis remains
unsatisfactory to the general public
despite recent advances.
Modern speech synthesizers can
produce natural-sounding speech,
but that speech is inflexible
and cannot be modified
to reflect the speaker's
emotional state or speaking style,
making it sounds prerecorded.
Synthesis techniques
that are more flexible exist,
but are often computationally intensive,
and often sound ``robotic,''
or too perfect,
due to the way in which
speech is produced.

Currently, the dominant speech synthesis
methods is concatenative (or unit selection) synthesis.
In concatenative synthesis, waveforms are generated
by selecting and concatenating audio samples
from a large database of recorded sounds
(``units,'' typically phonemes, but sometimes
smaller samples on the order of tens of milliseconds).
Since the units are recordings of real speech,
the result sounds natural.
Audio databases typically contain samples with
varying prosodic characteristics
(rhythm, stress, intonation)
allowing for words to be synthesized
using the underlying units
with the desired prosodic characteristics.
Machine learning techniques
are employed to efficiently search
the database of units and combine
the units together
(e.g., HMMs and Viterbi search,
as are used in speech recognition).
While the results are generally good
(concatenative synthesis is used in
the majority of commercially available
text-to-speech systems XXX) % http://www.theverge.com/2013/9/17/4596374/machine-language-how-siri-found-its-voice
this technique is limited to the corpus
of sounds available in the database.
Additionally, there can be discontinuities
in the transition from one unit
to the next, making even the recorded
voice sample sound artificial.
Attempts to modify the concatenated
samples after the fact,
to change its prosodic characteristics,
for example, often require
spectral transformations that also
result in unnatural-sounding speech.

Advances in computing power
have made statistical parametric synthesis
techniques viable.
While concatenative synthesis
cobbles together speech using existing speech,
statistical parametric synthesis
addresses speech synthesis
as a generative statistical problem,
much in the way that
the speech recognition formalism does.
Like concatenative synthesis,
a large database of recorded speech is necessary,
but instead of using
raw samples from that database,
the database is used to generate
a set of model parameters.
The problem of speech synthesis
is represented as
\begin{equation}
  \hat{o} = \arg \max_o P(o|W, \hat{\lambda}),
\end{equation}
where $o$ is a set of speech parameters,
$W$ is a sequence of words,
and $\hat{\lambda}$ is a set of model parameters
resulting from training on the speech database.
The appropriate speech parameters
for the given word sequence, $\hat{o}$,
is fed through a series of filters
to generate synthesized speech.

The most common method of implementing
statistical parametric synthesis
is to use an elaborate set of HMMs,
working at many different levels
and in parallel to model
all of the features necessary to produce
intelligible speech.
These features sets are significantly
more complicated than those
used in speech recognition;
in recognition, phonetic, prosodic,
and other contextual cues
can be collapsed into single HMM states,
whereas in synthesis,
all of these states must be explicitly tracked.
In one system (XXXZen2007),
there over a hundred parameters to be modeled.
Like similar techniques in speech recognition,
deep neural networks have begun
to replace HMMs, with promising results (XXXZen2013).
Despite these advances, concatenative systems
were still judged the most natural
at the 2012 Blizzard Challenge
(2013 results not available at this time).

\subsection{Articulatory speech synthesis}

The most flexible approach to speech synthesis
is articulatory speech synthesis.
Articulatory synthesis systems generate
audio waveforms by simulating
airflow through the human vocal tract.
While this method is
the most closely tied to biology,
and was used in the first fully automatic
text-to-speech synthesizer,
its computational complexity
and unnatural sounding results
have hindered its adoption
among academics and industry.
Additionally, accurate simulation
requires intricate knowledge
of how the human vocal tract
produces sounds,
which at present is incomplete.
However, unlike other methods,
articulatory speech synthesis
is able to test theories
resulting from the biological exploration
of sound production.

Generally, articulatory speech synthesizers
consist of three (typically independent) modules.
The first is a \textit{control model},
which generates vocal tract movements
by determining the desired positions
of the articulators in the vocal tract
(i.e., the degrees of freedom in the control problem).
The second is a \textit{vocal tract model},
which converts movements, or desired positions,
into vocal tract geometries in continuous time.
The third is an \textit{acoustic model},
which converts the geometries of the vocal tract
into acoustic signals (i.e., audio waveforms).
Each module has several classes of implementations.

XXX: some articulatory synth figure

One class of control models used in
articulatory speech synthesis
are segmental control models,
which work on a set of goals and constraints.
Given a phonological description
of the utterance to be synthesized,
segmental control models
produce a set of timestamps
(when a phoneme is to be voiced)
and a set of goals for some,
but not necessarily all,
of the articulators.
For example, to voice an \textit{a},
a segmental control model might say
that the \textit{a} starts at 0 ms
and ends at 100 ms,
and that the glottal aperture
must be at position 10
for that entire time window,
but that the tongue tip
should aim to reach position 0
at some point in that time window.
By specifying hard and soft constraints
on the vocal tract model,
segmental control can produce
realistic transitions between
phonemes simply by allowing
non-essential articulators to
remain in their current positions.

Another class of control models are
action based control models,
also known as gestural control models,
which define a set of discrete motor actions
(i.e., motor primitives in motor control literature)
that are combined to produce sounds according
to a phonemic and prosodic description.
This leads to several levels of description.
At the highest level,
the description to voice an \textit{a}
might be
``close the glottis
and open the velopharyngeal valve.''
This gets mapped down to a
high level motor plan,
which defines when the ``glottal closing''
and ``velopharyngeal valve opening''
actions occur and for how long.
This then is mapped down
to a lower level phonetic articulatory description
(using inverse dynamics)
that defines how each articulator
moves over time.

The control model is tightly coupled
with the vocal tract model.
The vocal tract model defines
the articulators that can be controlled,
and typically also provides some clue
for what articulator values
are appropriate for the
sound being synthesized.

One class of vocal tract models
are statistical models,
which extract relevant characteristics
from a large set of data,
mostly magnetic resonance imaging (MRI)
paired with audio recordings.
A full three-dimensional representation
of the patient's vocal tract
can be reconstructed with sufficient data,
and by using dimensionality reduction techniques
(e.g., XXXbaden2001orwhatever),
the articulatory control parameters
can be of tractable dimensionality
while still producing a full
3D representation of a vocal tract.

Another class of vocal tract models
are biomechanical models,
which attempt to fully reconstruct
the physiology of the vocal tract.
This type of model begins
like the statistical model,
using MRI and other anatomical measurements
to develop a 3D representation
of the vocal tract.
However, instead of statistically
reducing the dimensionality
to determine the degrees of freedom
available to the control system,
these models also model how
muscle activations affect the shape
of the vocal tract
and use that as the articulator.

A final class of vocal tract models
are geometric models,
which generalize the overall
shapes observed in
MRI and anatomical studies.
A set of parameters
are used to generate a 3D model
of a vocal tract,
which has a set of articulators.
While these models are typically
less detailed than those
generated from precise
measurements of one subject's vocal tract,
it can be used to generate vocal tracts
from different subjects
(both male and female).
The parameters of the model
can be tuned to a specific speaker,
though the resulting vocal tract
will not be as detailed.

Finally, the acoustic model takes
a vocal tract representation
and simulates the airflow from
the lungs, through the vocal tract,
to the air pressure coming out
of the mouth (i.e., the audio waveform).
These models typically relate
the vocal tract to some other physical
device that has been modeled previously.
One set of models treat the vocal tract
as an analog transmission line,
which has been used to
simulate arterial blood flow.
Another set of models treat the vocal tract
as an analog electrical circuit.
A final set of models,
called finite element wave propagation models,
explicitly models the air pressure wave
as it travels through each section
of the vocal tract.
Hybrid technique combine one
of the above techniques
with an analogous transformation
in the frequency domain for added accuracy.
Within these systems, the glottis
and the type of noise generated
by air turbulence can be changed
to modify the final synthesized audio.

\subsubsection{Birkholz synthesizer}

\subsection{Closed loop modelling}

Most modern motor control is based on
learning the control parameters via feedback.
Even the most sophisticated
articulatory speech synthesis systems
do not use feedback to improve control;
instead, they use very detailed measurements
of the control parameters
(aritculator positions).
This would be analogous to controlling
a robotic arm by measuring
the amount of muscle activation
for all of the muscles affecting
the end effector position.

One of the difficulties for speech in particular
is that the ``end effector'' being controlled
is typically mapped down
to a time-varying scalar.
Determining which of the dozens of control parameters
are responsible for a scalar error
is a near impossible task;
yet, humans are able to produce
exceedingly complex auditory signals
with their vocal tracts.
Perhaps gathering feedback
in the same way that the brain receives feedback
is key to how we learn to produce
complex sounds.

\subsection{Spiking neural networks}

\subsection{Neural Engineering Framework}

\subsection{Learning}

\section{Proposal}

Do adaptive control of
articulatory speech synthesis
using feedback from
an auditory periphery model.
This may involve the influence
of other elements
of the speech recognition pipeline,
hence their inclusion in this proposal.

\subsection{Speech recognition}

While the Zilany model
has closely replicated experimental data,
and has proven useful in cochlear implants,
it has not yet been used in the context
of automatic speech recognition.
There are several ways in which
the Zilany model could be used
to validate or improve automatic speech recognition.

First, the model could be compared
to existing methods used to map
audio waveforms to frames;
given many audio waveforms,
is the Zilany model able to
produce a representation that
is better suited for speech recognition
than existing methods
(e.g., a method may be better suited
if the distance in feature space
between different phonemes is larger).

Second, the Zilany model could
replace the feature extractor
in an existing speech recognition pipeline.
This may uncover some features
of the model that make it well suited
to speech recognition that
are difficult to reason about
given the model's output alone.

Third, the Zilany model could
act as the frontend to a
speech recognition system
fully realized in a spiking neural network.
The output of the Zilany model
is spike times;
when integrating with existing
speech recognition systems,
the spikes would either be
converted back to instantaneous rates,
or the spike generator step
would be omitted.
However, there may be significant performance
or efficiency gains to be had
by communicating with spikes
in a speech recognition system.

\subsection{Speech synthesis}

Can current vocal tract models give human-quality speech?

We'll try using modern motor control to generate target sounds.
We can use mngu0 sounds and fMRI for generating initial trajectories

\subsection{Closed loop modeling}

Can synthesized speech
influence auditory sensory learning,
and vice versa?

We'll try to hook them up together
and learn via babbling and outside teachers.

\subsection{Spiking neural networks}

Can the previously discussed items
be done in a biologically plausible manner?
Are there any practical benefits to doing this?

We'll use the NEF to model some aspects of this.
Some things,
like representing the articular parameters with neurons,
are trivial but provide good practical benefits
since computer speech sound mechanical due to no noise.


\end{document}
